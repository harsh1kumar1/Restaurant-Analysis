{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd73530-5d55-42f6-be49-78de498ad374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SQLAlchemy in c:\\users\\harsh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.32)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b21ef8-8094-4eff-a586-34c8bcec07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46375964-bc16-4e8d-acf9-728eae83447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150346, 14)\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file with the correct encoding\n",
    "with open('yelp_academic_dataset_business.json', 'r', encoding='utf-8') as f:\n",
    "    business_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "business_df = pd.DataFrame(business_data)\n",
    "\n",
    "print(business_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850568c0-ea7d-4f45-851d-19e5d30641fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkin DataFrame Shape: (131930, 2)\n"
     ]
    }
   ],
   "source": [
    "# # Open the JSON file with the correct encoding\n",
    "# with open('yelp_academic_dataset_checkin.json', 'r', encoding='utf-8') as f:\n",
    "#     checkin_data = [json.loads(line) for line in f]\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# checkin_df = pd.DataFrame(checkin_data)\n",
    "\n",
    "# print(checkin_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process and convert a chunk of JSON data to a DataFrame\n",
    "def process_chunk(chunk):\n",
    "    return pd.DataFrame(chunk)\n",
    "\n",
    "# Initialize an empty DataFrame to store checkin data\n",
    "checkin_df = pd.DataFrame()\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000  # Adjust this based on your memory capacity\n",
    "chunk = []\n",
    "\n",
    "# Open and read the JSON file in chunks\n",
    "with open('yelp_academic_dataset_checkin.json', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))\n",
    "        \n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            # Process and append the chunk to the DataFrame\n",
    "            checkin_df = pd.concat([checkin_df, process_chunk(chunk)], ignore_index=True)\n",
    "            chunk = []  # Reset chunk\n",
    "    \n",
    "    # Process the last chunk if it exists\n",
    "    if chunk:\n",
    "        checkin_df = pd.concat([checkin_df, process_chunk(chunk)], ignore_index=True)\n",
    "\n",
    "print('Checkin DataFrame Shape:', checkin_df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efb6736-042c-4bd3-8ee7-26b3f2efccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review DataFrame Shape: (3495140, 9)\n"
     ]
    }
   ],
   "source": [
    "# with open('yelp_academic_dataset_review.json', 'r', encoding='utf-8') as f:\n",
    "#     review_data = [json.loads(line) for line in f]\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# review_df = pd.DataFrame(review_data)\n",
    "\n",
    "\n",
    "# print(review_df.shape)\n",
    "\n",
    "\n",
    "# print(tip_df.shape)\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process and convert a chunk of JSON data to a DataFrame\n",
    "def process_chunk(chunk):\n",
    "    return pd.DataFrame(chunk)\n",
    "\n",
    "# Initialize an empty DataFrame to store review data\n",
    "review_df = pd.DataFrame()\n",
    "\n",
    "# Define chunk size and the maximum number of rows to read\n",
    "chunk_size = 10000  # Adjust this based on your memory capacity\n",
    "max_rows = 3495140  # Half of 6,990,280\n",
    "chunk = []\n",
    "rows_read = 0\n",
    "\n",
    "# Open and read the JSON file in chunks\n",
    "with open('yelp_academic_dataset_review.json', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))\n",
    "        rows_read += 1\n",
    "        \n",
    "        if rows_read >= max_rows:\n",
    "            # Process and append the chunk to the DataFrame\n",
    "            review_df = pd.concat([review_df, process_chunk(chunk)], ignore_index=True)\n",
    "            break  # Stop reading once we've reached the maximum number of rows\n",
    "        \n",
    "        if len(chunk) >= chunk_size:\n",
    "            # Process and append the chunk to the DataFrame\n",
    "            review_df = pd.concat([review_df, process_chunk(chunk)], ignore_index=True)\n",
    "            chunk = []  # Reset chunk\n",
    "\n",
    "    # Process any remaining data if rows_read is less than max_rows\n",
    "    if chunk and rows_read < max_rows:\n",
    "        review_df = pd.concat([review_df, process_chunk(chunk)], ignore_index=True)\n",
    "\n",
    "print('Review DataFrame Shape:', review_df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4feab43c-2768-48b5-875c-f3d1bbeeccd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
       "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
       "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    3.0       0      0     0   \n",
       "1    5.0       1      0     1   \n",
       "2    3.0       0      0     0   \n",
       "3    5.0       1      0     1   \n",
       "4    4.0       1      0     1   \n",
       "\n",
       "                                                text                 date  \n",
       "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n",
       "1  I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n",
       "2  Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
       "4  Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c68942-8e70-41ae-8888-39553a06399e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tip DataFrame Shape: (908915, 5)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process and convert a chunk of JSON data to a DataFrame\n",
    "def process_chunk(chunk):\n",
    "    return pd.DataFrame(chunk)\n",
    "\n",
    "# Initialize an empty DataFrame to store tip data\n",
    "tip_df = pd.DataFrame()\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000  # Adjust this based on your memory capacity\n",
    "chunk = []\n",
    "\n",
    "# Open and read the JSON file in chunks\n",
    "with open('yelp_academic_dataset_tip.json', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))\n",
    "        \n",
    "        if (i + 1) % chunk_size == 0:\n",
    "            # Process and append the chunk to the DataFrame\n",
    "            tip_df = pd.concat([tip_df, process_chunk(chunk)], ignore_index=True)\n",
    "            chunk = []  # Reset chunk\n",
    "    \n",
    "    # Process the last chunk if it exists\n",
    "    if chunk:\n",
    "        tip_df = pd.concat([tip_df, process_chunk(chunk)], ignore_index=True)\n",
    "\n",
    "print('Tip DataFrame Shape:', tip_df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3be7cd0-6094-4822-97a6-7caf672bc10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review DataFrame Shape: (993949, 22)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process and convert a chunk of JSON data to a DataFrame\n",
    "def process_chunk(chunk):\n",
    "    return pd.DataFrame(chunk)\n",
    "\n",
    "# Initialize an empty DataFrame to store review data\n",
    "user_df = pd.DataFrame()\n",
    "\n",
    "# Define chunk size and the maximum number of rows to read\n",
    "chunk_size = 10000  # Adjust this based on your memory capacity\n",
    "max_rows = 993949  # Half of 1,987,897\n",
    "chunk = []\n",
    "rows_read = 0\n",
    "\n",
    "# Open and read the JSON file in chunks\n",
    "with open('yelp_academic_dataset_user.json', 'r', encoding='utf-8') as f:  # Make sure to use the correct file\n",
    "    for i, line in enumerate(f):\n",
    "        chunk.append(json.loads(line))\n",
    "        rows_read += 1\n",
    "        \n",
    "        if rows_read >= max_rows:\n",
    "            # Process and append the chunk to the DataFrame\n",
    "            user_df = pd.concat([user_df, process_chunk(chunk)], ignore_index=True)\n",
    "            break  # Stop reading once we've reached the maximum number of rows\n",
    "        \n",
    "        if len(chunk) >= chunk_size:\n",
    "            user_df = pd.concat([user_df, process_chunk(chunk)], ignore_index=True)\n",
    "            chunk = []  # Reset chunk\n",
    "\n",
    "    # Process any remaining data if rows_read is less than max_rows\n",
    "    if chunk and rows_read < max_rows:\n",
    "        user_df = pd.concat([user_df, process_chunk(chunk)], ignore_index=True)\n",
    "\n",
    "print('Review DataFrame Shape:', user_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ae241c-7391-4930-ab18-0bbf97575684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>friends</th>\n",
       "      <th>fans</th>\n",
       "      <th>...</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_writer</th>\n",
       "      <th>compliment_photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n",
       "      <td>Walker</td>\n",
       "      <td>585</td>\n",
       "      <td>2007-01-25 16:47:26</td>\n",
       "      <td>7217</td>\n",
       "      <td>1259</td>\n",
       "      <td>5994</td>\n",
       "      <td>2007</td>\n",
       "      <td>NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>232</td>\n",
       "      <td>844</td>\n",
       "      <td>467</td>\n",
       "      <td>467</td>\n",
       "      <td>239</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>4333</td>\n",
       "      <td>2009-01-25 04:35:42</td>\n",
       "      <td>43091</td>\n",
       "      <td>13066</td>\n",
       "      <td>27281</td>\n",
       "      <td>2009,2010,2011,2012,2013,2014,2015,2016,2017,2...</td>\n",
       "      <td>ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...</td>\n",
       "      <td>3138</td>\n",
       "      <td>...</td>\n",
       "      <td>264</td>\n",
       "      <td>184</td>\n",
       "      <td>157</td>\n",
       "      <td>251</td>\n",
       "      <td>1847</td>\n",
       "      <td>7054</td>\n",
       "      <td>3131</td>\n",
       "      <td>3131</td>\n",
       "      <td>1521</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2WnXYQFK0hXEoTxPtV2zvg</td>\n",
       "      <td>Steph</td>\n",
       "      <td>665</td>\n",
       "      <td>2008-07-25 10:41:00</td>\n",
       "      <td>2086</td>\n",
       "      <td>1010</td>\n",
       "      <td>1003</td>\n",
       "      <td>2009,2010,2011,2012,2013</td>\n",
       "      <td>LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>96</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SZDeASXq7o05mMNLshsdIA</td>\n",
       "      <td>Gwen</td>\n",
       "      <td>224</td>\n",
       "      <td>2005-11-29 04:38:33</td>\n",
       "      <td>512</td>\n",
       "      <td>330</td>\n",
       "      <td>299</td>\n",
       "      <td>2009,2010,2011</td>\n",
       "      <td>enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hA5lMy-EnncsH4JoR-hFGQ</td>\n",
       "      <td>Karen</td>\n",
       "      <td>79</td>\n",
       "      <td>2007-01-05 19:40:59</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id    name  review_count        yelping_since  useful  \\\n",
       "0  qVc8ODYU5SZjKXVBgXdI7w  Walker           585  2007-01-25 16:47:26    7217   \n",
       "1  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333  2009-01-25 04:35:42   43091   \n",
       "2  2WnXYQFK0hXEoTxPtV2zvg   Steph           665  2008-07-25 10:41:00    2086   \n",
       "3  SZDeASXq7o05mMNLshsdIA    Gwen           224  2005-11-29 04:38:33     512   \n",
       "4  hA5lMy-EnncsH4JoR-hFGQ   Karen            79  2007-01-05 19:40:59      29   \n",
       "\n",
       "   funny   cool                                              elite  \\\n",
       "0   1259   5994                                               2007   \n",
       "1  13066  27281  2009,2010,2011,2012,2013,2014,2015,2016,2017,2...   \n",
       "2   1010   1003                           2009,2010,2011,2012,2013   \n",
       "3    330    299                                     2009,2010,2011   \n",
       "4     15      7                                                      \n",
       "\n",
       "                                             friends  fans  ...  \\\n",
       "0  NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...   267  ...   \n",
       "1  ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  3138  ...   \n",
       "2  LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...    52  ...   \n",
       "3  enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...    28  ...   \n",
       "4  PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...     1  ...   \n",
       "\n",
       "   compliment_more  compliment_profile  compliment_cute  compliment_list  \\\n",
       "0               65                  55               56               18   \n",
       "1              264                 184              157              251   \n",
       "2               13                  10               17                3   \n",
       "3                4                   1                6                2   \n",
       "4                1                   0                0                0   \n",
       "\n",
       "   compliment_note  compliment_plain  compliment_cool  compliment_funny  \\\n",
       "0              232               844              467               467   \n",
       "1             1847              7054             3131              3131   \n",
       "2               66                96              119               119   \n",
       "3               12                16               26                26   \n",
       "4                1                 1                0                 0   \n",
       "\n",
       "   compliment_writer  compliment_photos  \n",
       "0                239                180  \n",
       "1               1521               1946  \n",
       "2                 35                 18  \n",
       "3                 10                  9  \n",
       "4                  0                  0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3f42d24-7700-4e00-ac8b-91ce6af8c36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150346, 14)\n",
      "(131930, 2)\n",
      "(3495140, 9)\n",
      "(908915, 5)\n",
      "(993949, 22)\n"
     ]
    }
   ],
   "source": [
    "print(business_df.shape)\n",
    "print(checkin_df.shape)\n",
    "print(review_df.shape)\n",
    "print(tip_df.shape)\n",
    "print(user_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ac6682-1e59-4e5a-b39b-fcee80f2ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df.drop(['attributes','hours'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b348373a-6f15-46c6-9285-23f7a43a865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150346, 12)\n",
      "(131930, 2)\n",
      "(3495140, 9)\n",
      "(908915, 5)\n",
      "(993949, 22)\n"
     ]
    }
   ],
   "source": [
    "print(business_df.shape)\n",
    "print(checkin_df.shape)\n",
    "print(review_df.shape)\n",
    "print(tip_df.shape)\n",
    "print(user_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43399e04-062a-46df-9cd9-4622f1a0d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine that connects to a SQLite database named 'yelp.db'\n",
    "engine = create_engine('sqlite:///yelp_21.db')\n",
    "\n",
    "def load_dataframe(df, table_name, engine):\n",
    "    df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "load_dataframe(business_df,'business',engine)  \n",
    "load_dataframe(checkin_df,'checkin',engine) \n",
    "load_dataframe(review_df,'review',engine) \n",
    "load_dataframe(tip_df,'tip',engine) \n",
    "load_dataframe(user_df,'user',engine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b8a4a-5cce-48a9-885a-324b0b030f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
